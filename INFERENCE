encoder_model = Model(encoder_input, encoder_states)
decoder_state_input_h = Input(shape=(128,))
decoder_state_input_c = Input(shape=(128,))
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]
decoder_embedding_inf = Embedding(input_dim=output_dim, output_dim=64)
decoder_lstm_inf = LSTM(128, return_sequences=True, return_state=True)
decoder_output_inf, state_h, state_c = decoder_lstm_inf(decoder_embedding_inf(decoder_input), initial_state=decoder_states_inputs)
decoder_states = [state_h, state_c]
decoder_dense_inf = Dense(output_dim, activation='softmax')
decoder_output_inf = decoder_dense_inf(decoder_output_inf)
decoder_model = Model([decoder_input] + decoder_states_inputs, [decoder_output_inf] + decoder_states)
def generate_response(input_text):
    input_seq = tokenizer.texts_to_sequences([input_text])
    input_seq = pad_sequences(input_seq, maxlen=max_input_length, padding='post')
    states_value = encoder_model.predict(input_seq)
    target_seq = np.zeros((1, 1))
    target_seq[0, 0] = tokenizer.word_index['startseq']
    stop_condition = False
    response = ''
    while not stop_condition:
        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)
        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        sampled_word = tokenizer.index_word[sampled_token_index]
        response += ' ' + sampled_word
        if sampled_word == 'endseq' or len(response.split()) > max_output_length:
            stop_condition = True
        target_seq[0, 0] = sampled_token_index
        states_value = [h, c]
    return response.strip()
print(generate_response("Hi"))
